% Different types of dependence
The \ac{IID} assumption is ubiquitous in machine learning and statistics. When
using real data, the ubiquitous \ac{IID} assumption of the training examples
can be, and often is violated. For example, when samples are drawn from
different subjects, it is to be expected that samples drawn from the same
subject display similar (subject specific) variability, in other words, the uncertainty of
the observations decreases if the subject is known. 
%
The implications of classifying such non-\ac{IID} data has received
surprisingly little attention from the \ac{ML} community. Methods that deal with
conditionally \ac{IID} (i.e. for each label $y_i$, the corresponding instance
$\vec{x}_i$ is generated according to some fixed probability distribution
$P(\vec{x}_i | y_i)$) data do exist, such as for example \acp{HMM} and
\acp{CRF}. These models allow for structure between the \emph{labels}. Methods
for non-stationary data with direct dependencies between the observations
$\vec{x}_i$ have received considerably less attention.
%
With \ac{BCI} data, it is safe to assume that the labels are (relatively)
independent. The features derived from the \ac{EEG} however do display temporal
structure, and sessions (or users) can be considered as groups with different
feature distributions.

% Serial correlation - convolved data
A dependence over time is common in time series. This implies that when doing
prediction on a time series, knowing the error term of one data point is
informative of the error of the next. With this knowledge, better models for
time series can be built. This dependency is known as \emph{serial correlation}
in the field of econometrics, and is known to bias the standard error of
uncorrected regression coefficients to be biased downwards (overfitting).
Various methods do exist to estimate the first-order serial correlation
coefficient $\rho$ for consecutive error terms such as the iterative
Cochrane-Orcutt procedure or a grid-search method that minimizes the standard
error (Hildreth-Lu method) \cite{pindyck1991eme}. For linear regressors, a
second set of regressors based on lagged error terms can be used to correct the
problem (feasible \acl{GLS}).
 
% mixed effect models - groups
\begin{sloppypar}
In statistics, there are methods that model the dependencies within groups of
measurements. These so-called mixed-effect models separately model the
contribution of different users (known as fixed-effects) and the contribution
of random-effects. An example of the application of mixed-effect models to
inter-subject variability in \acp{BCI} can be found in \cite{fazli2011lpl}.
Since these mixed-effect methods are aimed at explanatory analyses, they rely
on estimating statistics for every group (user). While this improves the
understanding of the classification problem, they generally do not allow for
prediction of out-of-sample users since the fixed-effects of this user still
need to be estimated. The \ac{SOB} procedure presented in
Chapter~\ref{chap:sob} can be regarded as mixed-effects model, since the
adaptive whitener models and subtracts the fixed effects. The difference is
that the \ac{SOB} quickly re-estimates the fixed effects model for prediction.
\end{sloppypar}

The structure of prediction errors implies that not only the sum of the
errors, but also the distribution of the errors should be taken into account in
the optimization of the classifier, since related errors might lead to overfitting \cite{pindyck1991eme}. One approach to bias the classifier towards
certain distributions of errors is the \ac{QLYSVM} \cite{portera2004gql}. The
\ac{QLYSVM} uses a quadratic penalty for pairs for margin errors, and
optimizes 
\begin{align}
  \arg\min_{\vec{w},b,\xi} \quad
  & \frac{1}{2}\norm{\vec{w}}^2 
  + c \vec{\xi}^T Y S Y \vec{\xi}
\\
  \text{s.t.} \quad 
  & y_i \left(\vec{w}^T \vec{x}_i + b\right) + \vec{\xi}_i \ge 1
\\
  & \vec{\xi}_i \ge 0,
\end{align}
%
where $\vec{x}_i$ is the feature vector for instance $i$, $Y$ is a matrix with
the target labels $y_i \in \{-1, 1\}$ on its diagonal, $\vec{\xi}$ is a vector
with a positive slack variable for each training instance that allow support
vectors to penetrate the margin at a cost, and $S$ is a symmetric
positive-definite matrix that determines the cost of
pairs of margin errors.
%
The dual of this \ac{QLYSVM} can be solved by adding $\frac{1}{2c}S^{-1}$ to
the kernel matrix $K$ and solving the hard-margin \ac{SVM}'s dual. When $S$ is a
diagonal matrix, the \ac{QLYSVM} degenerates to the quadratic loss \ac{SVM}.
%
Unfortunately, the \ac{QLYSVM} cannot be used to exploit the decreasing
dependence with time: to reduce the weight of a temporary disturbance, the cost
of misclassifying two highly dependent instances should be less than the cost
of misclassifying two more independent instances. The resulting cost matrix $S$
is not positive definite, which prevents optimization of the \ac{QLYSVM}'s dual. Furthermore, the inversion of $S$ is expensive,
and might suffer from numerical inaccuracies depending on the kernel function
used to construct $S$.
